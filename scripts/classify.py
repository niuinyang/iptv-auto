#!/usr/bin/env python3
# coding: utf-8
import os, re, unicodedata, requests, json
from collections import defaultdict

# ------------------- é…ç½® -------------------
input_file = "output/working.m3u"
custom_multicast_url = "https://raw.githubusercontent.com/sumingyd/Telecom-Shandong-IPTV-List/refs/heads/main/Telecom-Shandong-Multicast.m3u"
custom_http_url = "https://raw.githubusercontent.com/sumingyd/Telecom-Shandong-IPTV-List/refs/heads/main/Telecom-Shandong.m3u"
output_dir = "output"
os.makedirs(output_dir, exist_ok=True)
os.makedirs("custom_m3u", exist_ok=True)
BASE_LOGO_RAW = "https://raw.githubusercontent.com/fanmingming/live/main/tv/"
new_gateway = "192.168.31.2"
new_port = "4022"
custom_multicast_file = os.path.join("custom_m3u", "Telecom-Shandong-Multicast-local.m3u")
name_map_file = "custom_m3u/name_map_auto.json"

# ------------------- å·¥å…·å‡½æ•° -------------------
def remove_control_chars(s): return ''.join(ch for ch in s if not unicodedata.category(ch).startswith('C'))
def remove_symbols_and_emoji(s): return ''.join(ch for ch in s if not unicodedata.category(ch).startswith('S'))
def normalize_spaces(s): return re.sub(r'\s+', ' ', s).strip()
def strip_tvg_fields(s): return re.sub(r'tvg-[a-zA-Z0-9_-]+="[^"]*"|group-title="[^"]*"', '', s)

def sanitize_title(extinf):
    s = strip_tvg_fields(extinf)
    if ',' in s:
        name = s.split(',', 1)[1].strip()
    else:
        m = re.search(r'tvg-name="([^"]+)"', s)
        name = m.group(1).strip() if m else re.sub(r'^#EXTINF[^,]*,?', '', s).strip()
    name = re.sub(r'["\u2000-\u206F\u2E00-\u2E7F\ufeff]', '', name)
    name = remove_symbols_and_emoji(remove_control_chars(name))
    return normalize_spaces(name) or "unknown"

def safe_logo_name(name):
    s = re.sub(r'[^\w\u4e00-\u9fff-]', '', remove_symbols_and_emoji(remove_control_chars(name)))
    return s[:60]

def build_logo_url(name):
    n = safe_logo_name(name)
    return BASE_LOGO_RAW + n + ".png"

# ------------------- ä¸‹è½½è‡ªå¤‡æº -------------------
def fetch_lines(url):
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        return r.text.splitlines()
    except Exception as e:
        print(f"âš ï¸ ä¸‹è½½å¤±è´¥ {url}: {e}")
        return []

def make_multicast_local(lines):
    out = []
    for l in lines:
        if l.startswith("http://") or l.startswith("https://"):
            l = re.sub(r"http://[\d\.]+:\d+(/rtp/.*)", f"http://{new_gateway}:{new_port}\\1", l)
        out.append(l)
    open(custom_multicast_file, "w", encoding="utf-8").write("\n".join(out))
    return out

def parse_pairs(lines, is_custom=False, source_type="other"):
    pairs = []
    for i in range(0, len(lines)-1):
        if lines[i].startswith("#EXTINF"):
            ext, url = lines[i].strip(), lines[i+1].strip()
            title = sanitize_title(ext)
            pairs.append((title, url, is_custom, source_type))
    return pairs

# ------------------- åˆ†ç±»å…³é”®å­— -------------------
categories = {
    "å¤®è§†": ["CCTV", "å¤®è§†"], 
    "å«è§†": ["å«è§†"], 
    "åœ°æ–¹": ["å±±ä¸œ","æ±Ÿè‹","æµ™æ±Ÿ","å¹¿ä¸œ","åŒ—äº¬","ä¸Šæµ·","å¤©æ´¥","æ¹–å—","é‡åº†","å››å·","æ¹–åŒ—","é™•è¥¿","ç¦å»º"],
    "æ¸¯å°": ["é¦™æ¸¯","TVB","å°æ¹¾","å°è§†","ä¸­è§†","ç¿¡ç¿ "], 
    "å›½é™…": ["BBC","CNN","NHK"], 
    "ç½‘ç»œé¢‘é“": ["æ–—é±¼","è™ç‰™","Bilibili"],
    "4Ké¢‘é“": []
}
category_order = ["å¤®è§†","å«è§†","åœ°æ–¹","æ¸¯å°","å›½é™…","ç½‘ç»œé¢‘é“","4Ké¢‘é“","å…¶ä»–"]
cctv_order = ["CCTV-1ç»¼åˆ","CCTV-2è´¢ç»","CCTV-3å¨±ä¹","CCTV-4ä¸­æ–‡å›½é™…","CCTV-5ä½“è‚²","CCTV-6ç”µå½±",
              "CCTV-7å›½é˜²å†›äº‹","CCTV-8ç”µè§†å‰§","CCTV-9çºªå½•","CCTV-10ç§‘æ•™","CCTV-11æˆæ›²",
              "CCTV-12ç¤¾ä¼šä¸æ³•","CCTV-13æ–°é—»","CCTV-14å°‘å„¿","CCTV-15éŸ³ä¹"]

group_title_map = {
    "å¤®è§†": "å¤®è§†é¢‘é“",
    "å«è§†": "å«è§†é¢‘é“",
    "åœ°æ–¹": "åœ°æ–¹é¢‘é“",
    "æ¸¯å°": "æ¸¯å°é¢‘é“",
    "å›½é™…": "å›½é™…é¢‘é“",
    "ç½‘ç»œé¢‘é“": "ç½‘ç»œé¢‘é“",
    "4Ké¢‘é“": "4Ké¢‘é“",
    "å…¶ä»–": "å…¶ä»–é¢‘é“"
}

# ------------------- æ™ºèƒ½ name_map + è‡ªåŠ¨æ›´æ–° -------------------
name_map = {
    "CCTV1":"CCTV-1ç»¼åˆ","å¤®è§†ç»¼åˆ":"CCTV-1ç»¼åˆ",
    "CCTV-2":"CCTV-2è´¢ç»","å¤®è§†è´¢ç»":"CCTV-2è´¢ç»",
    "CCTV-13":"CCTV-13æ–°é—»","å¤®è§†æ–°é—»":"CCTV-13æ–°é—»"
}
province_channels = {
    "å±±ä¸œå«è§†": ["å±±ä¸œå«è§†", "SDTV", "å±±ä¸œç”µè§†"],
    "æ±Ÿè‹å«è§†": ["æ±Ÿè‹å«è§†", "JSTV"],
    "æµ™æ±Ÿå«è§†": ["æµ™æ±Ÿå«è§†", "ZJTV"],
    "å¹¿ä¸œå«è§†": ["å¹¿ä¸œå«è§†", "GDTV"],
    "åŒ—äº¬å«è§†": ["åŒ—äº¬å«è§†", "BTV"],
    "ä¸Šæµ·ä¸œæ–¹å«è§†": ["ä¸œæ–¹å«è§†", "SHTV-DF"],
    "æ¹–å—å«è§†": ["æ¹–å—å«è§†", "HNTV"],
    "é‡åº†å«è§†": ["é‡åº†å«è§†", "CQTV"],
    "å››å·å«è§†": ["å››å·å«è§†", "SCTV"],
    "æ¹–åŒ—å«è§†": ["æ¹–åŒ—å«è§†", "HUBTV"],
    "é™•è¥¿å«è§†": ["é™•è¥¿å«è§†", "SXTV"],
    "ç¦å»ºä¸œå—å«è§†": ["ä¸œå—å«è§†", "FJTV"]
}
if os.path.exists(name_map_file):
    with open(name_map_file, "r", encoding="utf-8") as f:
        auto_name_map_dict = json.load(f)
else:
    auto_name_map_dict = {}
unmapped = set()

def smart_name_map(title):
    t = title.strip()
    if not t:
        return "unknown"
    if t in name_map: return name_map[t]
    if t in auto_name_map_dict: return auto_name_map_dict[t]

    t_clean = re.sub(r'(é«˜æ¸…|HD|æ ‡æ¸…|4K)', '', t, flags=re.I).replace(" ", "").replace("-", "")
    if t_clean.upper().startswith("CCTV") or t_clean.startswith("å¤®è§†"):
        m = re.search(r'\d+', t_clean)
        if m:
            num = m.group(0)
            mapping = {str(i): f"CCTV-{i}{suffix}" for i,suffix in 
                       enumerate(["ç»¼åˆ","è´¢ç»","å¨±ä¹","ä¸­æ–‡å›½é™…","ä½“è‚²","ç”µå½±","å›½é˜²å†›äº‹","ç”µè§†å‰§","çºªå½•","ç§‘æ•™","æˆæ›²","ç¤¾ä¼šä¸æ³•","æ–°é—»","å°‘å„¿","éŸ³ä¹"],1)}
            return mapping.get(num, t)
    t_upper = t_clean.upper()
    for std_name, aliases in province_channels.items():
        for a in aliases:
            a_clean = a.upper().replace(" ", "").replace("-", "")
            if a_clean in t_upper:
                return std_name
    unmapped.add(t)
    result = normalize_spaces(remove_symbols_and_emoji(remove_control_chars(t)))
    return result if result else "unknown"

# ------------------- åˆå¹¶æº -------------------
lines = open(input_file, encoding="utf-8").read().splitlines() if os.path.exists(input_file) else []
working_pairs = parse_pairs(lines, False, "other")
multicast_pairs = parse_pairs(make_multicast_local(fetch_lines(custom_multicast_url)), True, "multicast")
http_pairs = parse_pairs(fetch_lines(custom_http_url), True, "http")
merged = multicast_pairs + http_pairs + working_pairs

# ------------------- æ„å»ºé¢‘é“è¡¨ -------------------
channel_map = {c: defaultdict(list) for c in categories}
channel_map["å…¶ä»–"] = defaultdict(list)
custom_channels = set()
for title,url,is_custom,source_type in merged:
    std_name = smart_name_map(title)
    std_name = std_name or "unknown"
    is_4k = bool(re.search(r'4K', title, re.I))
    if is_4k: cat="4Ké¢‘é“"
    else:
        cat="å…¶ä»–"
        for c,kws in categories.items():
            if c=="4Ké¢‘é“": continue
            if any(kw.lower() in (std_name or "").lower() for kw in kws):
                cat=c
                break
    lst = channel_map[cat][std_name]
    if is_custom:
        custom_channels.add(std_name)
        if source_type=="multicast": lst.insert(0,url)
        elif source_type=="http":
            idx = next((i for i,v in enumerate(lst) if not v.startswith("http://"+new_gateway)), len(lst))
            lst.insert(idx,url)
    else: lst.append(url)

# ------------------- è¾“å‡º summary & åˆ†ç±»æ–‡ä»¶ -------------------
summary_lines=["#EXTM3U"]
for cat in category_order:
    keys=list(channel_map[cat].keys())
    if cat=="å¤®è§†": keys.sort(key=lambda x: cctv_order.index(x) if x in cctv_order else 999)
    else: keys.sort()
    keys.sort(key=lambda x: 0 if x in custom_channels else 1)
    path=os.path.join(output_dir,f"{cat}.m3u")
    with open(path,"w",encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        for k in keys:
            logo=build_logo_url(k)
            group_title=group_title_map.get(cat,cat)
            for u in channel_map[cat][k]:
                line=f'#EXTINF:-1 tvg-name="{k}" tvg-logo="{logo}" group-title="{group_title}",{k}'
                f.write(line+"\n"+u+"\n")
                summary_lines.append(line)
                summary_lines.append(u)

# ------------------- æˆäººé»‘åå• -------------------
def fetch_adult_blacklist():
    urls=[
        "https://raw.githubusercontent.com/emiliodallatorre/adult-hosts-list/main/list.txt",
        "https://raw.githubusercontent.com/columndeeply/hosts/main/hosts00",
        "https://raw.githubusercontent.com/Bon-Appetit/porn-domains/main/domains.txt"
    ]
    domains=set()
    for url in urls:
        try:
            r=requests.get(url,timeout=10); r.raise_for_status()
            for line in r.text.splitlines():
                line=line.strip()
                if line and not line.startswith("#"):
                    domain=re.sub(r'^0\.0\.0\.0\s+','',line)
                    domains.add(domain.lower())
        except Exception as e:
            print(f"âš ï¸ ä¸‹è½½æˆäººé»‘åå•å¤±è´¥: {url}, é”™è¯¯: {e}")
    return domains

adult_domains = fetch_adult_blacklist()
adult_channels=[]; clean_summary=[]
for i in range(0,len(summary_lines),2):
    if i+1>=len(summary_lines): break
    ext, url = summary_lines[i], summary_lines[i+1]
    if any(d in url.lower() for d in adult_domains):
        adult_channels.append((ext,url))
    else:
        clean_summary.append(ext); clean_summary.append(url)

# å†™å…¥ au.m3u
with open(os.path.join(output_dir,"au.m3u"),"w",encoding="utf-8") as f:
    f.write("#EXTM3U\n")
    for ext,url in adult_channels:
        f.write(ext+"\n"+url+"\n")

# è¦†ç›– summary.m3u
with open(os.path.join(output_dir,"summary.m3u"),"w",encoding="utf-8") as f:
    f.write("\n".join(clean_summary))

# ------------------- è‡ªåŠ¨æ›´æ–° name_map -------------------
if unmapped:
    auto_name_map_dict.update({k:k for k in unmapped})
    os.makedirs(os.path.dirname(name_map_file),exist_ok=True)
    with open(name_map_file,"w",encoding="utf-8") as f:
        json.dump(auto_name_map_dict,f,ensure_ascii=False,indent=2)
    print(f"ğŸ“ æ›´æ–°è‡ªåŠ¨ name_map æ–‡ä»¶: {name_map_file}, æ–°å¢ {len(unmapped)} ä¸ªæœªåŒ¹é…é¢‘é“")

print(f"âœ… classify.py æ‰§è¡Œå®Œæˆï¼Œsummaryã€4Kã€åˆ†ç±»æ–‡ä»¶ç”Ÿæˆï¼Œè‡ªå¤‡æºç½®é¡¶ï¼Œæˆäººæºåˆ†ç¦»å®Œæˆ")