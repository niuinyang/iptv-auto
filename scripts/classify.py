#!/usr/bin/env python3
# coding: utf-8
import os, re, unicodedata, requests, json
from collections import defaultdict

# ------------------- é…ç½® -------------------
input_file = "output/working.m3u"
custom_multicast_url = "https://raw.githubusercontent.com/sumingyd/Telecom-Shandong-IPTV-List/refs/heads/main/Telecom-Shandong-Multicast.m3u"
custom_http_url = "https://raw.githubusercontent.com/sumingyd/Telecom-Shandong-IPTV-List/refs/heads/main/Telecom-Shandong.m3u"
output_dir = "output"
os.makedirs(output_dir, exist_ok=True)
os.makedirs("custom_m3u", exist_ok=True)
BASE_LOGO_RAW = "https://raw.githubusercontent.com/fanmingming/live/main/tv/"
new_gateway = "192.168.31.2"
new_port = "4022"
custom_multicast_file = os.path.join("custom_m3u", "Telecom-Shandong-Multicast-local.m3u")
name_map_file = "custom_m3u/name_map_auto.json"

# ------------------- å·¥å…·å‡½æ•° -------------------
def remove_control_chars(s): return ''.join(ch for ch in s if not unicodedata.category(ch).startswith('C'))
def remove_symbols_and_emoji(s): return ''.join(ch for ch in s if not unicodedata.category(ch).startswith('S'))
def normalize_spaces(s): return re.sub(r'\s+', ' ', s).strip()
def strip_tvg_fields(s): return re.sub(r'tvg-[a-zA-Z0-9_-]+="[^"]*"|group-title="[^"]*"', '', s)

def sanitize_title(extinf):
    s = strip_tvg_fields(extinf)
    if ',' in s:
        name = s.split(',', 1)[1].strip()
    else:
        m = re.search(r'tvg-name="([^"]+)"', s)
        name = m.group(1).strip() if m else re.sub(r'^#EXTINF[^,]*,?', '', s).strip()
    name = re.sub(r'["\u2000-\u206F\u2E00-\u2E7F\ufeff]', '', name)
    name = remove_symbols_and_emoji(remove_control_chars(name))
    return normalize_spaces(name) or "unknown"

def safe_logo_name(name):
    s = re.sub(r'[^\w\u4e00-\u9fff-]', '', remove_symbols_and_emoji(remove_control_chars(name)))
    return s[:60]

def build_logo_url(name):
    n = safe_logo_name(name)
    return BASE_LOGO_RAW + n + ".png"

# ------------------- ä¸‹è½½è‡ªå¤‡æº -------------------
def fetch_lines(url):
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        return r.text.splitlines()
    except Exception as e:
        print(f"âš ï¸ ä¸‹è½½å¤±è´¥ {url}: {e}")
        return []

def make_multicast_local(lines):
    out = []
    for l in lines:
        if l.startswith("http://") or l.startswith("https://"):
            l = re.sub(r"http://[\d\.]+:\d+(/rtp/.*)", f"http://{new_gateway}:{new_port}\\1", l)
        out.append(l)
    open(custom_multicast_file, "w", encoding="utf-8").write("\n".join(out))
    return out

def parse_pairs(lines, is_custom=False, source_type="other"):
    pairs = []
    for i in range(0, len(lines)-1):
        if lines[i].startswith("#EXTINF"):
            ext, url = lines[i].strip(), lines[i+1].strip()
            title = sanitize_title(ext)
            pairs.append((title, url, is_custom, source_type))
    return pairs

multicast_pairs = parse_pairs(make_multicast_local(fetch_lines(custom_multicast_url)), True, "multicast")
http_pairs = parse_pairs(fetch_lines(custom_http_url), True, "http")

# ------------------- åˆ†ç±»å…³é”®å­— -------------------
categories = {
    "å¤®è§†": ["CCTV", "å¤®è§†"], 
    "å«è§†": ["å«è§†"], 
    "åœ°æ–¹": ["å±±ä¸œ","æ±Ÿè‹","æµ™æ±Ÿ","å¹¿ä¸œ","åŒ—äº¬","ä¸Šæµ·","å¤©æ´¥","æ¹–å—","é‡åº†","å››å·","æ¹–åŒ—","é™•è¥¿","ç¦å»º"],
    "æ¸¯å°": ["é¦™æ¸¯","TVB","å°æ¹¾","å°è§†","ä¸­è§†","ç¿¡ç¿ "], 
    "å›½é™…": ["BBC","CNN","NHK"], 
    "ç½‘ç»œé¢‘é“": ["æ–—é±¼","è™ç‰™","Bilibili"],
    "4Ké¢‘é“": []
}
category_order = ["å¤®è§†","å«è§†","åœ°æ–¹","æ¸¯å°","å›½é™…","ç½‘ç»œé¢‘é“","4Ké¢‘é“","å…¶ä»–"]
cctv_order = ["CCTV-1ç»¼åˆ","CCTV-2è´¢ç»","CCTV-3å¨±ä¹","CCTV-4ä¸­æ–‡å›½é™…","CCTV-5ä½“è‚²","CCTV-6ç”µå½±",
              "CCTV-7å›½é˜²å†›äº‹","CCTV-8ç”µè§†å‰§","CCTV-9çºªå½•","CCTV-10ç§‘æ•™","CCTV-11æˆæ›²",
              "CCTV-12ç¤¾ä¼šä¸æ³•","CCTV-13æ–°é—»","CCTV-14å°‘å„¿","CCTV-15éŸ³ä¹"]

group_title_map = {
    "å¤®è§†": "å¤®è§†é¢‘é“",
    "å«è§†": "å«è§†é¢‘é“",
    "åœ°æ–¹": "åœ°æ–¹é¢‘é“",
    "æ¸¯å°": "æ¸¯å°é¢‘é“",
    "å›½é™…": "å›½é™…é¢‘é“",
    "ç½‘ç»œé¢‘é“": "ç½‘ç»œé¢‘é“",
    "4Ké¢‘é“": "4Ké¢‘é“",
    "å…¶ä»–": "å…¶ä»–é¢‘é“"
}

# ------------------- æ™ºèƒ½ name_map + è‡ªåŠ¨æ›´æ–° -------------------
name_map = {
    "CCTV1":"CCTV-1ç»¼åˆ","å¤®è§†ç»¼åˆ":"CCTV-1ç»¼åˆ",
    "CCTV-2":"CCTV-2è´¢ç»","å¤®è§†è´¢ç»":"CCTV-2è´¢ç»",
    "CCTV-13":"CCTV-13æ–°é—»","å¤®è§†æ–°é—»":"CCTV-13æ–°é—»"
}

province_channels = {
    "å±±ä¸œå«è§†": ["å±±ä¸œå«è§†", "SDTV", "å±±ä¸œç”µè§†"],
    "æ±Ÿè‹å«è§†": ["æ±Ÿè‹å«è§†", "JSTV"],
    "æµ™æ±Ÿå«è§†": ["æµ™æ±Ÿå«è§†", "ZJTV"],
    "å¹¿ä¸œå«è§†": ["å¹¿ä¸œå«è§†", "GDTV"],
    "åŒ—äº¬å«è§†": ["åŒ—äº¬å«è§†", "BTV"],
    "ä¸Šæµ·ä¸œæ–¹å«è§†": ["ä¸œæ–¹å«è§†", "SHTV-DF"],
    "æ¹–å—å«è§†": ["æ¹–å—å«è§†", "HNTV"],
    "é‡åº†å«è§†": ["é‡åº†å«è§†", "CQTV"],
    "å››å·å«è§†": ["å››å·å«è§†", "SCTV"],
    "æ¹–åŒ—å«è§†": ["æ¹–åŒ—å«è§†", "HUBTV"],
    "é™•è¥¿å«è§†": ["é™•è¥¿å«è§†", "SXTV"],
    "ç¦å»ºä¸œå—å«è§†": ["ä¸œå—å«è§†", "FJTV"]
}

if os.path.exists(name_map_file):
    with open(name_map_file, "r", encoding="utf-8") as f:
        auto_name_map_dict = json.load(f)
else:
    auto_name_map_dict = {}

unmapped = set()

def smart_name_map(title):
    t = title.strip()
    if not t:
        return "unknown"
    if t in name_map:
        return name_map[t]
    if t in auto_name_map_dict:
        return auto_name_map_dict[t]

    t_clean = re.sub(r'(é«˜æ¸…|HD|æ ‡æ¸…|4K)', '', t, flags=re.I).replace(" ", "").replace("-", "")

    if t_clean.upper().startswith("CCTV") or t_clean.startswith("å¤®è§†"):
        m = re.search(r'\d+', t_clean)
        if m:
            num = m.group(0)
            mapping = {
                "1":"CCTV-1ç»¼åˆ","2":"CCTV-2è´¢ç»","3":"CCTV-3å¨±ä¹","4":"CCTV-4ä¸­æ–‡å›½é™…",
                "5":"CCTV-5ä½“è‚²","6":"CCTV-6ç”µå½±","7":"CCTV-7å›½é˜²å†›äº‹","8":"CCTV-8ç”µè§†å‰§",
                "9":"CCTV-9çºªå½•","10":"CCTV-10ç§‘æ•™","11":"CCTV-11æˆæ›²","12":"CCTV-12ç¤¾ä¼šä¸æ³•",
                "13":"CCTV-13æ–°é—»","14":"CCTV-14å°‘å„¿","15":"CCTV-15éŸ³ä¹"
            }
            return mapping.get(num, t)

    t_upper = t_clean.upper()
    for std_name, aliases in province_channels.items():
        for a in aliases:
            a_clean = a.upper().replace(" ", "").replace("-", "")
            if a_clean in t_upper:
                return std_name

    unmapped.add(t)
    result = normalize_spaces(remove_symbols_and_emoji(remove_control_chars(t)))
    if not result:
        result = "unknown"
    return result

# ------------------- åˆå¹¶æ‰€æœ‰æº -------------------
lines = open(input_file, encoding="utf-8").read().splitlines() if os.path.exists(input_file) else []
working_pairs = parse_pairs(lines, False, "other")
merged = multicast_pairs + http_pairs + working_pairs

# ------------------- æ„å»ºé¢‘é“è¡¨ï¼ˆä¸¥æ ¼ç»„æ’­ç½®é¡¶ï¼‰ -------------------
channel_map = {c: defaultdict(list) for c in categories}
channel_map["å…¶ä»–"] = defaultdict(list)
custom_channels = set()

for title, url, is_custom, source_type in merged:
    std_name = smart_name_map(title)
    std_name = std_name or "unknown"
    is_4k = bool(re.search(r'4K', title, re.I))
    if is_4k:
        cat = "4Ké¢‘é“"
    else:
        cat = "å…¶ä»–"
        for c, kws in categories.items():
            if c == "4Ké¢‘é“":
                continue
            if any(kw.lower() in (std_name or "").lower() for kw in kws):
                cat = c
                break

    lst = channel_map[cat][std_name]
    if is_custom:
        custom_channels.add(std_name)
        if source_type=="multicast":
            lst.insert(0, url)  # è‡ªå¤‡ç»„æ’­æœ€å‰
        elif source_type=="http":
            idx = next((i for i,v in enumerate(lst) if not v.startswith("http://"+new_gateway)), len(lst))
            lst.insert(idx, url)
    else:
        lst.append(url)  # å…¶ä»–æºæœ€å

# ------------------- è¾“å‡º summary & åˆ†ç±»æ–‡ä»¶ -------------------
summary_lines = ["#EXTM3U"]
for cat in category_order:
    keys = list(channel_map[cat].keys())
    if cat == "å¤®è§†":
        keys.sort(key=lambda x: cctv_order.index(x) if x in cctv_order else 999)
    else:
        keys.sort()

    path = os.path.join(output_dir, f"{cat}.m3u")
    with open(path, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        for k in keys:
            logo = build_logo_url(k)
            group_title = group_title_map.get(cat, cat)
            for u in channel_map[cat][k]:
                line = f'#EXTINF:-1 tvg-name="{k}" tvg-logo="{logo}" group-title="{group_title}",{k}'
                f.write(line + "\n" + u + "\n")
                summary_lines.append(line)
                summary_lines.append(u)

# ------------------- æˆäººé»‘åå•æ£€æµ‹ -------------------
adult_domains = [
    "baddiehub.com",
    "porn",
    "xvideos",
    "xnxx",
    "adult",
    "sex"
]

adult_channels = []
clean_summary = []

for i in range(0, len(summary_lines), 2):
    line_extinf = summary_lines[i]
    line_url = summary_lines[i+1]
    if any(domain.lower() in line_url.lower() for domain in adult_domains):
        adult_channels.append((line_extinf, line_url))
    else:
        clean_summary.append(line_extinf)
        clean_summary.append(line_url)

# å†™å…¥ au.m3u
with open(os.path.join(output_dir, "au.m3u"), "w", encoding="utf-8") as f:
    f.write("#EXTM3U\n")
    for extinf, url in adult_channels:
        f.write(extinf + "\n" + url + "\n")

# è¦†ç›– summary.m3uï¼Œåˆ é™¤æˆäººæº
with open(os.path.join(output_dir,"summary.m3u"), "w", encoding="utf-8") as f:
    f.write("\n".join(clean_summary))

# ------------------- è‡ªåŠ¨æ›´æ–° name_map_auto.json -------------------
if unmapped:
    auto_name_map_dict.update({k:k for k in unmapped})
    os.makedirs(os.path.dirname(name_map_file), exist_ok=True)
    with open(name_map_file, "w", encoding="utf-8") as f:
        json.dump(auto_name_map_dict, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ æ›´æ–°è‡ªåŠ¨ name_map æ–‡ä»¶: {name_map_file}, æ–°å¢ {len(unmapped)} ä¸ªæœªåŒ¹é…é¢‘é“")

print(f"âœ… classify.py æ‰§è¡Œå®Œæˆï¼šé¢‘é“åˆ†ç±»ã€æ’åºã€ç»„æ’­ç½®é¡¶ã€è‡ªå¤‡æºä¼˜å…ˆã€summary è¾“å‡ºå®Œæˆ")
print(f"âœ… æˆäººæºæå–å®Œæˆï¼Œå…± {len(adult_channels)} ä¸ªæˆäººé¢‘é“ï¼Œç”Ÿæˆ au.m3u å¹¶æ›´æ–° summary.m3u")